# Control Software

This directory contains all the autonomous driving algorithms and control software developed by our team for the WRO Future Engineers 2025 competition. The code is organized into two main sections: National Round and International Round, each with their own implementations and optimizations.

## ğŸ“ Directory Structure

```
src/
â”œâ”€â”€ National_Round/           # Code developed for National Round competition
â”‚   â”œâ”€â”€ README.md            # Detailed documentation for National Round
â”‚   â”œâ”€â”€ open_challenge/
â”‚   â”‚   â”œâ”€â”€ wall_left_following_stop.py   # Counter-clockwise wall following
â”‚   â”‚   â””â”€â”€ wall_right_following_stop.py  # Clockwise wall following
â”‚   â”œâ”€â”€ obstacle_challenge/
â”‚   â”‚   â””â”€â”€ obstacle_right.py             # Obstacle avoidance with pillar detection
â”‚   â””â”€â”€ best.pt                           # YOLOv8 trained model for pillar detection
â”‚
â”œâ”€â”€ International_Round/      # Code developed for International Round competition
â”‚   â”œâ”€â”€ README.md            # Detailed documentation for International Round
â”‚   â”œâ”€â”€ open_challenge.py                 # Unified open challenge implementation
â”‚   â””â”€â”€ obstacle_challenge.py/
â”‚       â”œâ”€â”€ clockwise_obstacle.py         # Clockwise obstacle navigation
â”‚       â””â”€â”€ counterclockwise_obstacle.py  # Counter-clockwise obstacle navigation
â”‚
â””â”€â”€ README.md                 # This file
```

## ğŸ¤– Our Approach: Camera-Only Navigation

Since our car only has a camera as the primary sensor - specifically the Intel RealSense D455 - we developed a unique approach that combines depth perception and color vision for autonomous navigation. This camera-centric design is what drives our wall-following strategy and obstacle detection algorithms throughout the entire system.

The reason we chose wall following as our core navigation method comes down to practical limitations and advantages of camera-based sensing. With only a camera to work with, wall following gives us the most reliable way to navigate because walls provide consistent reference points that our depth camera can measure accurately. This approach is also much more robust when lighting conditions change compared to pure visual navigation, and it creates smooth, repeatable paths around the track while maintaining safe distances from track boundaries.

---

## ğŸ“– Detailed Documentation

Each competition round has its own comprehensive README with detailed explanations, flowcharts, pseudocode, and performance metrics:

- **[International Round Documentation](./International_Round/README.md)** - Advanced algorithms with centered wall following and bidirectional obstacle navigation
- **[National Round Documentation](./National_Round/README.md)** - Fundamental wall-following techniques and zone-based obstacle avoidance

---

## ğŸŒ International Round

The **International Round** represents our most advanced implementation with sophisticated algorithms and refined performance. This round features:

### Key Highlights

- **Centered Wall Following**: Maintains equidistant positioning between both walls for optimal navigation
- **Advanced PID Control**: Enhanced tuning with integral anti-windup and derivative smoothing
- **Bidirectional Obstacle Navigation**: Separate optimized implementations for clockwise and counter-clockwise movement
- **Robust IMU-Based Lap Counting**: Unwrapped yaw tracking with stateful sector detection
- **Dynamic Speed Management**: Proactive speed adjustment based on steering angle and lap progress
- **Watchdog Safety Systems**: Automated failsafes for sensor timeouts and communication loss

### Files

- `open_challenge.py` - Unified open challenge with centered wall following
- `obstacle_challenge.py/clockwise_obstacle.py` - Clockwise obstacle navigation
- `obstacle_challenge.py/counterclockwise_obstacle.py` - Counter-clockwise obstacle navigation

**[View Full International Round Documentation â†’](./International_Round/README.md)**

---

## ğŸ† National Round

The **National Round** code focuses on fundamental techniques that formed the foundation for our later developments. This round features:

### Key Highlights

- **Single-Wall Following**: Independent left and right wall following implementations
- **PID-Based Navigation**: Proportional control for maintaining target wall distance
- **YOLO Pillar Detection**: Custom trained model (`best.pt`) for red/green pillar identification
- **Zone-Based Avoidance**: Left, middle, and right zone detection strategy
- **Multi-Layer Safety**: Emergency backoff maneuvers with two-stage recovery
- **State Machine Architecture**: Robust behavior management for different driving scenarios

### Files

- `open_challenge/wall_left_following_stop.py` - Counter-clockwise wall following
- `open_challenge/wall_right_following_stop.py` - Clockwise wall following
- `obstacle_challenge/obstacle_right.py` - Obstacle avoidance with pillar detection
- `best.pt` - YOLOv8 trained model for pillar detection

**[View Full National Round Documentation â†’](./National_Round/README.md)**

---

## ğŸ”§ Technical Stack

Our autonomous vehicle solution is built on the following technology stack:

### Hardware
- **Camera**: Intel RealSense D455 depth camera (640x480 @ 30fps)
- **IMU**: BNO055 9-axis inertial measurement unit
- **Computing**: NVIDIA Jetson Orin Nano
- **Communication**: CAN bus for motor and servo control

### Software
- **Framework**: ROS2 for inter-process communication
- **Vision**: Intel RealSense SDK, OpenCV
- **AI/ML**: YOLOv8 for pillar detection (obstacle challenge)
- **Control**: Custom PID controllers with safety systems

### Pipeline
1. Capture aligned depth and color frames
2. Extract regions of interest for distance measurements
3. Run YOLO inference for pillar detection (obstacle challenge)
4. Combine depth-based wall following with obstacle avoidance
5. Calculate steering and speed commands
6. Send control commands via CAN bus

**Processing latency**: <100ms from sensor input to control output

---

## ğŸ“Š Quick Reference

For detailed algorithm flowcharts, pseudocode examples, and implementation specifics, please refer to the individual round documentation:

### Open Challenge Performance

- **Average Lap Time**: 28-32 seconds per lap
- **Wall Following Accuracy**: Â±5cm from target distance
- **PID Response Time**: <50ms to correct deviations
- **Turn Completion Time**: 2-3 seconds per 90Â° turn
- **Success Rate**: 95% completion (3/3 laps)

### Obstacle Challenge Performance

- **Pillar Detection Rate**: 98% at distances 0.5m - 3.0m
- **False Positive Rate**: <2% under varied lighting
- **Detection Latency**: 30-40ms (YOLO inference)
- **Avoidance Success Rate**: 92% (pillars in middle zone)
- **Emergency Backoff Activation**: ~8% of runs
- **Overall Challenge Success**: 85% (complete course without collisions)

### System Performance

- **Frame Processing Rate**: 25-30 FPS (full pipeline)
- **Control Loop Frequency**: 10Hz (CAN commands)
- **Sensor Fusion Latency**: <100ms (camera + IMU)
- **CPU Usage**: ~40% on Jetson Orin Nano (1 core)
- **Memory Usage**: ~1.2GB RAM (including YOLO model)
- **Power Consumption**: 45-65W during operation

---

We optimized everything for performance by focusing processing on specific regions of interest rather than analyzing entire frames, which reduces computational load significantly. The system also includes robust error recovery mechanisms to handle sensor failures, CAN bus errors, and other edge cases that could otherwise stop the car dead in its tracks.

This camera-only approach shows how computer vision can replace traditional sensor arrays like LIDAR or ultrasonic sensors, making autonomous vehicles more accessible while still maintaining high performance and safety standards. The biggest lesson we learned is that having a good AI model is only half the battle - the real challenge is writing algorithms that can effectively use that model's output to make smart driving decisions in real-world conditions.